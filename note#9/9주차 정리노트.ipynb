{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==46==\n",
    "매개변수 갱신 작업(갱신 코드)를 모듈화하고 다른 모듈로 대체할 수 있는 코드 만들기\n",
    "\n",
    "**Optimizer클래스**\n",
    "매개변수 갱신을 위한 기반 클래스."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setup 메서드는 매개변수를 갖는 클래스를 인스턴스 변수인 target으로 설정\n",
    "# def setup(self, target):\n",
    "#         self.target = target\n",
    "#         return self\n",
    "\n",
    "# #update 메서드는 모든 매개변수를 갱신\n",
    "# def update(self):\n",
    "#         params = [p for p in self.target.params() if p.grad is not None]\n",
    "\n",
    "#         for f in self.hooks:\n",
    "#             f(params)\n",
    "\n",
    "#         for param in params:\n",
    "#             self.update_one(param)\n",
    "\n",
    "# # SGD 클래스\n",
    "# # 경사하강법으로 매개변수를 갱신\n",
    "# # __init__메서드는 학습률을 받아 초기화\n",
    "# #update_one메서드에서 매개변수 갱신 코드를 구현\n",
    "# class SGD(Optimizer):\n",
    "#     def __init__(self, lr=0.01):\n",
    "#         super().__init__()\n",
    "#         self.lr = lr\n",
    "\n",
    "#     def update_one(self, param):\n",
    "#         param.data -= self.lr * param.grad.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD 클래스를 사용한 문제해결\n",
    "from dezero import optimizers\n",
    "\n",
    "# model = MLP((hidden_size, 1)) #MLP클래스를 사용하여 모델을 생성\n",
    "# optimizer = Optimizers.SGD(lr).setup(model)\n",
    "\n",
    "# optimizer.update() #SGD클래스로 매개변수를 갱신"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SGD 이외의 최적화 기법\n",
    "기울기를 이용한 최적화 기법\n",
    "-Momentum, AdaGrad, AdaDelta, Adam\n",
    "-Optimizer클래스를 이용하고 상속하여 최적화 기법을 필요에 따라 전환하거나 구현.\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "W는 갱신할 가중치 매개변수,v는 속도, 𝜕𝐿/𝜕W는 기울기,  η는 학습률\n",
    "αv는 아무런 힘을 받지않을 때 서서히 감속시키는 역할\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #MomentumSGD 구현 코드\n",
    "# #속도에 해당하는 데이터, 딕셔너리 타임의 인스턴스 변수 self.vs에 유지\n",
    "# #초기화 시에는 vs에 아무것도 담겨있지 않다.\n",
    "\n",
    "# class MomentumSGD(Optimizer):\n",
    "#     def __init__(self, lr=0.01, momentum=0.9):\n",
    "#         super().__init__()\n",
    "#         self.lr = lr\n",
    "#         self.momentum = momentum\n",
    "#         self.vs = {} \n",
    "\n",
    "#     def update_one(self, param):\n",
    "#         v_key = id(param)\n",
    "#         if v_key not in self.vs:\n",
    "#             xp = cuda.get_array_module(param.data)\n",
    "#             self.vs[v_key] = xp.zeros_like(param.data)\n",
    "\n",
    "#         v = self.vs[v_key]\n",
    "#         v *= self.momentum\n",
    "#         v -= self.lr * param.grad.data\n",
    "#         param.data += v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==47==\n",
    "다중 클래스 분류\n",
    "-여러 클래스로 분류하는 문제\n",
    "-분류 대상이 여러가지 클래스 중 어디에 속하는지 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([4 5 6])\n"
     ]
    }
   ],
   "source": [
    "# 슬라이스 조작 함수\n",
    "# Variable의 다차원 배열 중에서 일부를 슬라이스하여 뽑아준다.\n",
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "y = F.get_item(x,1) #(2,3)형상의 x에서 1번째 행의 원소 추출\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[1 2 3]\n",
      "          [1 2 3]\n",
      "          [4 5 6]])\n"
     ]
    }
   ],
   "source": [
    "#다차원 배열의 일부를 추출하는 작업\n",
    "#get_item 함수 사용시 같은 인덱스를 반복 지정하여 동일원소 여러번 뺄 수 있다.\n",
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "indices = np.array([0,0,1])\n",
    "y = F.get_item(x,indices)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([4 5 6])\n",
      "variable([3 6])\n"
     ]
    }
   ],
   "source": [
    "# 특수 메서드로 설정\n",
    "# get_item함수를 Variable메서드로 사용함.\n",
    "# 슬라이스 작업의 역전파도 이뤄진다.\n",
    "\n",
    "Variable.__getitem__ = F.get_item\n",
    "y = x[1]\n",
    "print(y)\n",
    "\n",
    "y = x[: , 2]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[ 0.3871487  -0.50755447 -1.40892602]])\n"
     ]
    }
   ],
   "source": [
    "# 소프트맥스 함수\n",
    "## 신경망으로 다중 클래스 분류\n",
    "# 선형회귀 때 이용한 신경망 & MLP 클래스로 구현해둔 신경망을 그대로 이용할 수 있다.\n",
    "\n",
    "from dezero.models import MLP\n",
    "model = MLP((10,3)) # 2층으로 이뤄진 완전연결 신경망을 만들어준다 >>  원소가 3개인 3차원 벡터로 변환\n",
    "\n",
    "x = np.array([[0.2, -0.4]]) #x의 형상은 (1,2) > 원소가 2개인 2차원 벡터\n",
    "y = model(x)\n",
    "print(y) #값이 가장 큰 원소의 인덱스가 이 모델이 분류한 클래스. > 2번 원소가 크기때문에 2번 클래스로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[ 0.3871487  -0.50755447 -1.40892602]])\n",
      "variable([[0.6350505 0.2595635 0.105386 ]])\n"
     ]
    }
   ],
   "source": [
    "from dezero import Variable, as_variable\n",
    "import dezero.functions as F\n",
    "\n",
    "#소프트맥스 함수의 입력 𝑦k가 총 n개라고 가정\n",
    "#exp(yk)는 입력 yk의 지수함수\n",
    "def softmax1d(x):\n",
    "    x = as_variable(x)\n",
    "    y = F.exp(x)\n",
    "    sum_y = F.sum(y)\n",
    "    return y / sum_y\n",
    "\n",
    "x = Variable(np.array([[0.2, -0.4]]))\n",
    "y = model(x)\n",
    "p = softmax1d(y)\n",
    "print(y)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치batch데이터에서도 소프트맥스 함수를 적용할 수 있다.\n",
    "# 샘플 데이터 각각에 소프트맥스 함수를 적용\n",
    "\n",
    "# def softmax_simple(x, axis=1): # 인수 x는 2차원 데이터로 가정\n",
    "#     x = as_variable(x)\n",
    "#     y = exp(x)\n",
    "#     sum_y = sum(y, axis=axis, keepdims=True) # True이므로 각 행에서 나눗셈이 이뤄진다.\n",
    "#     return y / sum_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형 회귀에선 손실 함수로 평균 제곱 오차를 이용.\n",
    "# 다중 클래스 분류에 적합한 손실함수는 \"교차 엔트로피 오차\"를 이용.\n",
    "\n",
    "-정답 데이터의 원손느 정답에 해당하는 클래스면 1로, 그렇지 않으면 0으로\n",
    "-표현 방식을 원핫 벡터라한다.\n",
    "-pk는 신경망에서 소프트맥스 함수를 적용한 후의 출력\n",
    "-정답 클래스에 해당하는 번호의 확률 p를 추출함으로써 교차 엔트로피의 오차를 계산\n",
    "-P[t]는 벡터 p에서 t번째 요소만을 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax_cross_entropy_simple(x, t):\n",
    "#     x, t = as_variable(x), as_variable(t) #x는 신경망에서 소프트맥스 함수 적용 전의 출력, t는 정답 데이터\n",
    "#     N = x.shape[0]\n",
    "\n",
    "#     p = softmax(x) # 또는 softmax_simple(x)\n",
    "#     p = clip(p, 1e-15, 1.0)  # log(0)을 방지하기 위해 p의 값을 1e-15 이상으로\n",
    "#     log_p = log(p) #log는 Dezero함수\n",
    "#     tlog_p = log_p[np.arange(N), t.data]\n",
    "#     y = -1 * sum(tlog_p) / N\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(1.201275546926755)\n"
     ]
    }
   ],
   "source": [
    "#다중 클래스 분류를 하는 신경망에 구체적인 데이터를 주어 교차 엔트로피 오차 계산\n",
    "x = np.array([[0.2, -0.4],[0.3,0.5],[1.3,-3.2],[2.1,0.3]])\n",
    "t = np.array([2, 0, 1, 0])\n",
    "y = model(x)\n",
    "loss = F.softmax_cross_entropy_simple(y,t) # 혹은 F.softmax_cross_entropy(y,t) # 손실함수 계산\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==48==\n",
    "다중 클래스 분류 수행\n",
    "-소프트맥스 함수와 교차 엔트로피 오차를 구현\n",
    "-스파이럴 데이터셋(나선형or소용돌이 모양)이라는 작은 데이터셋을 사용하여 다중 클래스 분류 실제 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n",
      "(300,)\n",
      "[-0.12995958 -0.00324155] 1\n",
      "[ 0.3282343  -0.54941994] 0\n"
     ]
    }
   ],
   "source": [
    "import dezero.datasets as ds\n",
    "x, t = ds.get_spiral(train = True)\n",
    "print(x.shape)\n",
    "print(t.shape)\n",
    "\n",
    "print(x[10], t[10])\n",
    "print(x[110], t[110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치: 데이터가 많을 때는 조금씩 무작위로 모아서 처리 이때의 '데이터 뭉치'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.13\n",
      "epoch 2, loss 1.05\n",
      "epoch 3, loss 0.95\n",
      "epoch 4, loss 0.92\n",
      "epoch 5, loss 0.87\n",
      "epoch 6, loss 0.89\n",
      "epoch 7, loss 0.84\n",
      "epoch 8, loss 0.78\n",
      "epoch 9, loss 0.80\n",
      "epoch 10, loss 0.79\n",
      "epoch 11, loss 0.78\n",
      "epoch 12, loss 0.76\n",
      "epoch 13, loss 0.77\n",
      "epoch 14, loss 0.76\n",
      "epoch 15, loss 0.76\n",
      "epoch 16, loss 0.77\n",
      "epoch 17, loss 0.78\n",
      "epoch 18, loss 0.74\n",
      "epoch 19, loss 0.74\n",
      "epoch 20, loss 0.72\n",
      "epoch 21, loss 0.73\n",
      "epoch 22, loss 0.74\n",
      "epoch 23, loss 0.77\n",
      "epoch 24, loss 0.73\n",
      "epoch 25, loss 0.74\n",
      "epoch 26, loss 0.74\n",
      "epoch 27, loss 0.72\n",
      "epoch 28, loss 0.72\n",
      "epoch 29, loss 0.72\n",
      "epoch 30, loss 0.73\n",
      "epoch 31, loss 0.71\n",
      "epoch 32, loss 0.72\n",
      "epoch 33, loss 0.72\n",
      "epoch 34, loss 0.71\n",
      "epoch 35, loss 0.72\n",
      "epoch 36, loss 0.71\n",
      "epoch 37, loss 0.71\n",
      "epoch 38, loss 0.70\n",
      "epoch 39, loss 0.71\n",
      "epoch 40, loss 0.70\n",
      "epoch 41, loss 0.71\n",
      "epoch 42, loss 0.70\n",
      "epoch 43, loss 0.70\n",
      "epoch 44, loss 0.70\n",
      "epoch 45, loss 0.69\n",
      "epoch 46, loss 0.69\n",
      "epoch 47, loss 0.71\n",
      "epoch 48, loss 0.70\n",
      "epoch 49, loss 0.69\n",
      "epoch 50, loss 0.69\n",
      "epoch 51, loss 0.68\n",
      "epoch 52, loss 0.67\n",
      "epoch 53, loss 0.68\n",
      "epoch 54, loss 0.70\n",
      "epoch 55, loss 0.68\n",
      "epoch 56, loss 0.66\n",
      "epoch 57, loss 0.67\n",
      "epoch 58, loss 0.66\n",
      "epoch 59, loss 0.64\n",
      "epoch 60, loss 0.64\n",
      "epoch 61, loss 0.64\n",
      "epoch 62, loss 0.63\n",
      "epoch 63, loss 0.63\n",
      "epoch 64, loss 0.61\n",
      "epoch 65, loss 0.61\n",
      "epoch 66, loss 0.60\n",
      "epoch 67, loss 0.62\n",
      "epoch 68, loss 0.59\n",
      "epoch 69, loss 0.60\n",
      "epoch 70, loss 0.57\n",
      "epoch 71, loss 0.58\n",
      "epoch 72, loss 0.57\n",
      "epoch 73, loss 0.56\n",
      "epoch 74, loss 0.56\n",
      "epoch 75, loss 0.55\n",
      "epoch 76, loss 0.55\n",
      "epoch 77, loss 0.55\n",
      "epoch 78, loss 0.54\n",
      "epoch 79, loss 0.53\n",
      "epoch 80, loss 0.53\n",
      "epoch 81, loss 0.52\n",
      "epoch 82, loss 0.53\n",
      "epoch 83, loss 0.52\n",
      "epoch 84, loss 0.49\n",
      "epoch 85, loss 0.50\n",
      "epoch 86, loss 0.49\n",
      "epoch 87, loss 0.49\n",
      "epoch 88, loss 0.48\n",
      "epoch 89, loss 0.47\n",
      "epoch 90, loss 0.47\n",
      "epoch 91, loss 0.46\n",
      "epoch 92, loss 0.46\n",
      "epoch 93, loss 0.45\n",
      "epoch 94, loss 0.44\n",
      "epoch 95, loss 0.45\n",
      "epoch 96, loss 0.44\n",
      "epoch 97, loss 0.43\n",
      "epoch 98, loss 0.43\n",
      "epoch 99, loss 0.42\n",
      "epoch 100, loss 0.43\n",
      "epoch 101, loss 0.42\n",
      "epoch 102, loss 0.41\n",
      "epoch 103, loss 0.42\n",
      "epoch 104, loss 0.41\n",
      "epoch 105, loss 0.40\n",
      "epoch 106, loss 0.40\n",
      "epoch 107, loss 0.40\n",
      "epoch 108, loss 0.39\n",
      "epoch 109, loss 0.38\n",
      "epoch 110, loss 0.39\n",
      "epoch 111, loss 0.38\n",
      "epoch 112, loss 0.38\n",
      "epoch 113, loss 0.38\n",
      "epoch 114, loss 0.36\n",
      "epoch 115, loss 0.36\n",
      "epoch 116, loss 0.36\n",
      "epoch 117, loss 0.36\n",
      "epoch 118, loss 0.36\n",
      "epoch 119, loss 0.35\n",
      "epoch 120, loss 0.35\n",
      "epoch 121, loss 0.36\n",
      "epoch 122, loss 0.34\n",
      "epoch 123, loss 0.35\n",
      "epoch 124, loss 0.33\n",
      "epoch 125, loss 0.33\n",
      "epoch 126, loss 0.32\n",
      "epoch 127, loss 0.34\n",
      "epoch 128, loss 0.32\n",
      "epoch 129, loss 0.33\n",
      "epoch 130, loss 0.31\n",
      "epoch 131, loss 0.30\n",
      "epoch 132, loss 0.31\n",
      "epoch 133, loss 0.31\n",
      "epoch 134, loss 0.30\n",
      "epoch 135, loss 0.29\n",
      "epoch 136, loss 0.29\n",
      "epoch 137, loss 0.29\n",
      "epoch 138, loss 0.28\n",
      "epoch 139, loss 0.29\n",
      "epoch 140, loss 0.28\n",
      "epoch 141, loss 0.27\n",
      "epoch 142, loss 0.27\n",
      "epoch 143, loss 0.28\n",
      "epoch 144, loss 0.27\n",
      "epoch 145, loss 0.27\n",
      "epoch 146, loss 0.26\n",
      "epoch 147, loss 0.26\n",
      "epoch 148, loss 0.26\n",
      "epoch 149, loss 0.26\n",
      "epoch 150, loss 0.25\n",
      "epoch 151, loss 0.25\n",
      "epoch 152, loss 0.25\n",
      "epoch 153, loss 0.24\n",
      "epoch 154, loss 0.24\n",
      "epoch 155, loss 0.24\n",
      "epoch 156, loss 0.24\n",
      "epoch 157, loss 0.24\n",
      "epoch 158, loss 0.24\n",
      "epoch 159, loss 0.23\n",
      "epoch 160, loss 0.23\n",
      "epoch 161, loss 0.23\n",
      "epoch 162, loss 0.23\n",
      "epoch 163, loss 0.23\n",
      "epoch 164, loss 0.22\n",
      "epoch 165, loss 0.22\n",
      "epoch 166, loss 0.22\n",
      "epoch 167, loss 0.21\n",
      "epoch 168, loss 0.22\n",
      "epoch 169, loss 0.22\n",
      "epoch 170, loss 0.21\n",
      "epoch 171, loss 0.21\n",
      "epoch 172, loss 0.22\n",
      "epoch 173, loss 0.22\n",
      "epoch 174, loss 0.21\n",
      "epoch 175, loss 0.21\n",
      "epoch 176, loss 0.20\n",
      "epoch 177, loss 0.21\n",
      "epoch 178, loss 0.20\n",
      "epoch 179, loss 0.20\n",
      "epoch 180, loss 0.20\n",
      "epoch 181, loss 0.20\n",
      "epoch 182, loss 0.19\n",
      "epoch 183, loss 0.20\n",
      "epoch 184, loss 0.19\n",
      "epoch 185, loss 0.19\n",
      "epoch 186, loss 0.19\n",
      "epoch 187, loss 0.19\n",
      "epoch 188, loss 0.19\n",
      "epoch 189, loss 0.19\n",
      "epoch 190, loss 0.19\n",
      "epoch 191, loss 0.19\n",
      "epoch 192, loss 0.19\n",
      "epoch 193, loss 0.18\n",
      "epoch 194, loss 0.19\n",
      "epoch 195, loss 0.18\n",
      "epoch 196, loss 0.18\n",
      "epoch 197, loss 0.18\n",
      "epoch 198, loss 0.18\n",
      "epoch 199, loss 0.19\n",
      "epoch 200, loss 0.18\n",
      "epoch 201, loss 0.17\n",
      "epoch 202, loss 0.18\n",
      "epoch 203, loss 0.18\n",
      "epoch 204, loss 0.17\n",
      "epoch 205, loss 0.18\n",
      "epoch 206, loss 0.17\n",
      "epoch 207, loss 0.17\n",
      "epoch 208, loss 0.17\n",
      "epoch 209, loss 0.17\n",
      "epoch 210, loss 0.17\n",
      "epoch 211, loss 0.17\n",
      "epoch 212, loss 0.17\n",
      "epoch 213, loss 0.18\n",
      "epoch 214, loss 0.17\n",
      "epoch 215, loss 0.17\n",
      "epoch 216, loss 0.17\n",
      "epoch 217, loss 0.17\n",
      "epoch 218, loss 0.17\n",
      "epoch 219, loss 0.16\n",
      "epoch 220, loss 0.17\n",
      "epoch 221, loss 0.16\n",
      "epoch 222, loss 0.16\n",
      "epoch 223, loss 0.16\n",
      "epoch 224, loss 0.16\n",
      "epoch 225, loss 0.16\n",
      "epoch 226, loss 0.16\n",
      "epoch 227, loss 0.17\n",
      "epoch 228, loss 0.18\n",
      "epoch 229, loss 0.16\n",
      "epoch 230, loss 0.16\n",
      "epoch 231, loss 0.15\n",
      "epoch 232, loss 0.16\n",
      "epoch 233, loss 0.17\n",
      "epoch 234, loss 0.16\n",
      "epoch 235, loss 0.16\n",
      "epoch 236, loss 0.15\n",
      "epoch 237, loss 0.16\n",
      "epoch 238, loss 0.16\n",
      "epoch 239, loss 0.16\n",
      "epoch 240, loss 0.16\n",
      "epoch 241, loss 0.15\n",
      "epoch 242, loss 0.15\n",
      "epoch 243, loss 0.15\n",
      "epoch 244, loss 0.15\n",
      "epoch 245, loss 0.15\n",
      "epoch 246, loss 0.15\n",
      "epoch 247, loss 0.15\n",
      "epoch 248, loss 0.15\n",
      "epoch 249, loss 0.15\n",
      "epoch 250, loss 0.15\n",
      "epoch 251, loss 0.15\n",
      "epoch 252, loss 0.15\n",
      "epoch 253, loss 0.15\n",
      "epoch 254, loss 0.15\n",
      "epoch 255, loss 0.15\n",
      "epoch 256, loss 0.15\n",
      "epoch 257, loss 0.14\n",
      "epoch 258, loss 0.15\n",
      "epoch 259, loss 0.14\n",
      "epoch 260, loss 0.15\n",
      "epoch 261, loss 0.15\n",
      "epoch 262, loss 0.15\n",
      "epoch 263, loss 0.14\n",
      "epoch 264, loss 0.14\n",
      "epoch 265, loss 0.14\n",
      "epoch 266, loss 0.14\n",
      "epoch 267, loss 0.14\n",
      "epoch 268, loss 0.14\n",
      "epoch 269, loss 0.14\n",
      "epoch 270, loss 0.14\n",
      "epoch 271, loss 0.14\n",
      "epoch 272, loss 0.14\n",
      "epoch 273, loss 0.14\n",
      "epoch 274, loss 0.14\n",
      "epoch 275, loss 0.14\n",
      "epoch 276, loss 0.14\n",
      "epoch 277, loss 0.14\n",
      "epoch 278, loss 0.14\n",
      "epoch 279, loss 0.14\n",
      "epoch 280, loss 0.13\n",
      "epoch 281, loss 0.13\n",
      "epoch 282, loss 0.14\n",
      "epoch 283, loss 0.13\n",
      "epoch 284, loss 0.13\n",
      "epoch 285, loss 0.13\n",
      "epoch 286, loss 0.13\n",
      "epoch 287, loss 0.14\n",
      "epoch 288, loss 0.13\n",
      "epoch 289, loss 0.13\n",
      "epoch 290, loss 0.13\n",
      "epoch 291, loss 0.13\n",
      "epoch 292, loss 0.13\n",
      "epoch 293, loss 0.14\n",
      "epoch 294, loss 0.13\n",
      "epoch 295, loss 0.13\n",
      "epoch 296, loss 0.13\n",
      "epoch 297, loss 0.13\n",
      "epoch 298, loss 0.12\n",
      "epoch 299, loss 0.13\n",
      "epoch 300, loss 0.13\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import dezero\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "import numpy as np\n",
    "# 하이퍼파라미터 생성(사람이 결정하는 매개변수, 은닉층 수와 학습률 등이 여기 속함)\n",
    "max_epoch = 300 # 에포크는 일종의 단위 (준비된 데이터셋 모두 사용시 1에포크)\n",
    "batch_size = 30 # 데이터를 한번에 30개씩 묶어 처리\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "#데이터 읽기/ 모델, 옵티마이저 생성\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(x)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # 데이터셋의 인덱스 뒤섞기\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # 미니배치 생성\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_x = x[batch_index]\n",
    "        batch_t = t[batch_index]\n",
    "\n",
    "        # 기울기 산출/ 매개변수 갱신\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # 에포크마다 학습 경과 출력\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 실행 후 손실 그래프\n",
    "\n",
    "-학습을 진행할수록 손실이 줄어든다\n",
    "-딥러닝의 특징은 층을 더 깊게 쌓는 식으로 표현력을 키울 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==49==\n",
    "\n",
    "대규모 데이터셋 처리\n",
    "-스파이럴 데이터셋은 작은 데이터셋이라서 ndarray 인스턴스 하나로 처리\n",
    "-Dataset클래스에는 데이터를 전처리할 수 있는 구조도 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset클래스는 기반 클래스로서의 역할을 한다.\n",
    "#실제로 사용하는 데이터셋은 이를 상속하여 구현\n",
    "# class Dataset:\n",
    "#     def __init__(self, train=True):\n",
    "#         self.train = train\n",
    "#         self.data = None\n",
    "#         self.label = None\n",
    "#         self.prepare()\n",
    "\n",
    "#     def __getitem__(self, index): #단순히 지정된 인덱스에 위치하는 데이터를 꺼낸다.\n",
    "#         assert np.isscalar(index) #index는 정수만 지원\n",
    "#         if self.label is None:\n",
    "#             return self.transform(self.data[index]), None\n",
    "#         else:\n",
    "#             return self.transform(self.data[index])\n",
    "\n",
    "#     def __len__(self): #데이터셋의 길이를 알려준다.\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def prepare(self):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Spiral(Dataset):\n",
    "#     def prepare(self):\n",
    "#         self.data, self.label = get_spiral(self.train)\n",
    "# >>데이터 추출할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.13981389, -0.00721657], dtype=float32), 1)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dezero\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "print(train_set[0])\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **큰 데이터셋 처리**\n",
    "# 작은 데이터셋은 Dataset클래스의 인스턴스 변수인 data와 label에 직접 ndarray 인스턴스를 유지해도 무리가 없다.\n",
    "## 빅데이터 처리 방법\n",
    "    # BigData 클래스를 초기화할 때는 데이터를 읽지 않는다.\n",
    "    # 데이터에 접근하는 __getitem__(index)가 불리는 시점에 데이터를 읽는다.\n",
    "# class Bigdata(Dataset):\n",
    "#     def __getitem__(index):\n",
    "#         x = np.load('data/[].npy'.format(index))\n",
    "#         t = np.load('label/[].npy'.format(index))\n",
    "#         return x,t\n",
    "#     def __len__():\n",
    "#         return 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "#데이터 이어 붙이기\n",
    "\n",
    "#신경망 입력 형태로의 데이터를 준비\n",
    "#인덱스를 지정하여 batch에 여러 데이터가 리스트로 저장하고 ndarray 인스턴스로 변환\n",
    "\n",
    "import dezero.datasets\n",
    "\n",
    "\n",
    "train_set = dezero.datasets.Spiral()\n",
    "\n",
    "#batch의 각 원소에서 데이터만을 꺼내 하나의 ndarray 인스턴스로 변형함(이어붙임)\n",
    "batch_index = [0,1, 2]\n",
    "batch = [train_set[i] for i in batch_index]\n",
    "\n",
    "x = np.array([example[0] for example in batch])\n",
    "t = np.array([example[1] for example in batch])\n",
    "\n",
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.13\n",
      "epoch 2, loss 1.05\n",
      "epoch 3, loss 0.95\n",
      "epoch 4, loss 0.92\n",
      "epoch 5, loss 0.87\n",
      "epoch 6, loss 0.89\n",
      "epoch 7, loss 0.84\n",
      "epoch 8, loss 0.78\n",
      "epoch 9, loss 0.80\n",
      "epoch 10, loss 0.79\n",
      "epoch 11, loss 0.78\n",
      "epoch 12, loss 0.76\n",
      "epoch 13, loss 0.77\n",
      "epoch 14, loss 0.76\n",
      "epoch 15, loss 0.76\n",
      "epoch 16, loss 0.77\n",
      "epoch 17, loss 0.78\n",
      "epoch 18, loss 0.74\n",
      "epoch 19, loss 0.74\n",
      "epoch 20, loss 0.72\n",
      "epoch 21, loss 0.73\n",
      "epoch 22, loss 0.74\n",
      "epoch 23, loss 0.77\n",
      "epoch 24, loss 0.73\n",
      "epoch 25, loss 0.74\n",
      "epoch 26, loss 0.74\n",
      "epoch 27, loss 0.72\n",
      "epoch 28, loss 0.72\n",
      "epoch 29, loss 0.72\n",
      "epoch 30, loss 0.73\n",
      "epoch 31, loss 0.71\n",
      "epoch 32, loss 0.72\n",
      "epoch 33, loss 0.72\n",
      "epoch 34, loss 0.71\n",
      "epoch 35, loss 0.72\n",
      "epoch 36, loss 0.71\n",
      "epoch 37, loss 0.71\n",
      "epoch 38, loss 0.70\n",
      "epoch 39, loss 0.71\n",
      "epoch 40, loss 0.70\n",
      "epoch 41, loss 0.71\n",
      "epoch 42, loss 0.70\n",
      "epoch 43, loss 0.70\n",
      "epoch 44, loss 0.70\n",
      "epoch 45, loss 0.69\n",
      "epoch 46, loss 0.69\n",
      "epoch 47, loss 0.71\n",
      "epoch 48, loss 0.70\n",
      "epoch 49, loss 0.69\n",
      "epoch 50, loss 0.69\n",
      "epoch 51, loss 0.68\n",
      "epoch 52, loss 0.67\n",
      "epoch 53, loss 0.68\n",
      "epoch 54, loss 0.70\n",
      "epoch 55, loss 0.68\n",
      "epoch 56, loss 0.66\n",
      "epoch 57, loss 0.67\n",
      "epoch 58, loss 0.66\n",
      "epoch 59, loss 0.64\n",
      "epoch 60, loss 0.64\n",
      "epoch 61, loss 0.64\n",
      "epoch 62, loss 0.63\n",
      "epoch 63, loss 0.63\n",
      "epoch 64, loss 0.61\n",
      "epoch 65, loss 0.61\n",
      "epoch 66, loss 0.60\n",
      "epoch 67, loss 0.62\n",
      "epoch 68, loss 0.59\n",
      "epoch 69, loss 0.60\n",
      "epoch 70, loss 0.57\n",
      "epoch 71, loss 0.58\n",
      "epoch 72, loss 0.57\n",
      "epoch 73, loss 0.56\n",
      "epoch 74, loss 0.56\n",
      "epoch 75, loss 0.55\n",
      "epoch 76, loss 0.55\n",
      "epoch 77, loss 0.55\n",
      "epoch 78, loss 0.54\n",
      "epoch 79, loss 0.53\n",
      "epoch 80, loss 0.53\n",
      "epoch 81, loss 0.52\n",
      "epoch 82, loss 0.53\n",
      "epoch 83, loss 0.52\n",
      "epoch 84, loss 0.49\n",
      "epoch 85, loss 0.50\n",
      "epoch 86, loss 0.49\n",
      "epoch 87, loss 0.49\n",
      "epoch 88, loss 0.48\n",
      "epoch 89, loss 0.47\n",
      "epoch 90, loss 0.47\n",
      "epoch 91, loss 0.46\n",
      "epoch 92, loss 0.46\n",
      "epoch 93, loss 0.45\n",
      "epoch 94, loss 0.44\n",
      "epoch 95, loss 0.45\n",
      "epoch 96, loss 0.44\n",
      "epoch 97, loss 0.43\n",
      "epoch 98, loss 0.43\n",
      "epoch 99, loss 0.42\n",
      "epoch 100, loss 0.43\n",
      "epoch 101, loss 0.42\n",
      "epoch 102, loss 0.41\n",
      "epoch 103, loss 0.42\n",
      "epoch 104, loss 0.41\n",
      "epoch 105, loss 0.40\n",
      "epoch 106, loss 0.40\n",
      "epoch 107, loss 0.40\n",
      "epoch 108, loss 0.39\n",
      "epoch 109, loss 0.38\n",
      "epoch 110, loss 0.39\n",
      "epoch 111, loss 0.38\n",
      "epoch 112, loss 0.38\n",
      "epoch 113, loss 0.38\n",
      "epoch 114, loss 0.36\n",
      "epoch 115, loss 0.36\n",
      "epoch 116, loss 0.36\n",
      "epoch 117, loss 0.36\n",
      "epoch 118, loss 0.36\n",
      "epoch 119, loss 0.35\n",
      "epoch 120, loss 0.35\n",
      "epoch 121, loss 0.36\n",
      "epoch 122, loss 0.34\n",
      "epoch 123, loss 0.35\n",
      "epoch 124, loss 0.33\n",
      "epoch 125, loss 0.33\n",
      "epoch 126, loss 0.32\n",
      "epoch 127, loss 0.34\n",
      "epoch 128, loss 0.32\n",
      "epoch 129, loss 0.33\n",
      "epoch 130, loss 0.31\n",
      "epoch 131, loss 0.30\n",
      "epoch 132, loss 0.31\n",
      "epoch 133, loss 0.31\n",
      "epoch 134, loss 0.30\n",
      "epoch 135, loss 0.29\n",
      "epoch 136, loss 0.29\n",
      "epoch 137, loss 0.29\n",
      "epoch 138, loss 0.28\n",
      "epoch 139, loss 0.29\n",
      "epoch 140, loss 0.28\n",
      "epoch 141, loss 0.27\n",
      "epoch 142, loss 0.27\n",
      "epoch 143, loss 0.28\n",
      "epoch 144, loss 0.27\n",
      "epoch 145, loss 0.27\n",
      "epoch 146, loss 0.26\n",
      "epoch 147, loss 0.26\n",
      "epoch 148, loss 0.26\n",
      "epoch 149, loss 0.26\n",
      "epoch 150, loss 0.25\n",
      "epoch 151, loss 0.25\n",
      "epoch 152, loss 0.25\n",
      "epoch 153, loss 0.24\n",
      "epoch 154, loss 0.24\n",
      "epoch 155, loss 0.24\n",
      "epoch 156, loss 0.24\n",
      "epoch 157, loss 0.24\n",
      "epoch 158, loss 0.24\n",
      "epoch 159, loss 0.23\n",
      "epoch 160, loss 0.23\n",
      "epoch 161, loss 0.23\n",
      "epoch 162, loss 0.23\n",
      "epoch 163, loss 0.23\n",
      "epoch 164, loss 0.22\n",
      "epoch 165, loss 0.22\n",
      "epoch 166, loss 0.22\n",
      "epoch 167, loss 0.21\n",
      "epoch 168, loss 0.22\n",
      "epoch 169, loss 0.22\n",
      "epoch 170, loss 0.21\n",
      "epoch 171, loss 0.21\n",
      "epoch 172, loss 0.22\n",
      "epoch 173, loss 0.22\n",
      "epoch 174, loss 0.21\n",
      "epoch 175, loss 0.21\n",
      "epoch 176, loss 0.20\n",
      "epoch 177, loss 0.21\n",
      "epoch 178, loss 0.20\n",
      "epoch 179, loss 0.20\n",
      "epoch 180, loss 0.20\n",
      "epoch 181, loss 0.20\n",
      "epoch 182, loss 0.19\n",
      "epoch 183, loss 0.20\n",
      "epoch 184, loss 0.19\n",
      "epoch 185, loss 0.19\n",
      "epoch 186, loss 0.19\n",
      "epoch 187, loss 0.19\n",
      "epoch 188, loss 0.19\n",
      "epoch 189, loss 0.19\n",
      "epoch 190, loss 0.19\n",
      "epoch 191, loss 0.19\n",
      "epoch 192, loss 0.19\n",
      "epoch 193, loss 0.18\n",
      "epoch 194, loss 0.19\n",
      "epoch 195, loss 0.18\n",
      "epoch 196, loss 0.18\n",
      "epoch 197, loss 0.18\n",
      "epoch 198, loss 0.18\n",
      "epoch 199, loss 0.19\n",
      "epoch 200, loss 0.18\n",
      "epoch 201, loss 0.17\n",
      "epoch 202, loss 0.18\n",
      "epoch 203, loss 0.18\n",
      "epoch 204, loss 0.17\n",
      "epoch 205, loss 0.18\n",
      "epoch 206, loss 0.17\n",
      "epoch 207, loss 0.17\n",
      "epoch 208, loss 0.17\n",
      "epoch 209, loss 0.17\n",
      "epoch 210, loss 0.17\n",
      "epoch 211, loss 0.17\n",
      "epoch 212, loss 0.17\n",
      "epoch 213, loss 0.18\n",
      "epoch 214, loss 0.17\n",
      "epoch 215, loss 0.17\n",
      "epoch 216, loss 0.17\n",
      "epoch 217, loss 0.17\n",
      "epoch 218, loss 0.17\n",
      "epoch 219, loss 0.16\n",
      "epoch 220, loss 0.17\n",
      "epoch 221, loss 0.16\n",
      "epoch 222, loss 0.16\n",
      "epoch 223, loss 0.16\n",
      "epoch 224, loss 0.16\n",
      "epoch 225, loss 0.16\n",
      "epoch 226, loss 0.16\n",
      "epoch 227, loss 0.17\n",
      "epoch 228, loss 0.18\n",
      "epoch 229, loss 0.16\n",
      "epoch 230, loss 0.16\n",
      "epoch 231, loss 0.15\n",
      "epoch 232, loss 0.16\n",
      "epoch 233, loss 0.17\n",
      "epoch 234, loss 0.16\n",
      "epoch 235, loss 0.16\n",
      "epoch 236, loss 0.15\n",
      "epoch 237, loss 0.16\n",
      "epoch 238, loss 0.16\n",
      "epoch 239, loss 0.16\n",
      "epoch 240, loss 0.16\n",
      "epoch 241, loss 0.15\n",
      "epoch 242, loss 0.15\n",
      "epoch 243, loss 0.15\n",
      "epoch 244, loss 0.15\n",
      "epoch 245, loss 0.15\n",
      "epoch 246, loss 0.15\n",
      "epoch 247, loss 0.15\n",
      "epoch 248, loss 0.15\n",
      "epoch 249, loss 0.15\n",
      "epoch 250, loss 0.15\n",
      "epoch 251, loss 0.15\n",
      "epoch 252, loss 0.15\n",
      "epoch 253, loss 0.15\n",
      "epoch 254, loss 0.15\n",
      "epoch 255, loss 0.15\n",
      "epoch 256, loss 0.15\n",
      "epoch 257, loss 0.14\n",
      "epoch 258, loss 0.15\n",
      "epoch 259, loss 0.14\n",
      "epoch 260, loss 0.15\n",
      "epoch 261, loss 0.15\n",
      "epoch 262, loss 0.15\n",
      "epoch 263, loss 0.14\n",
      "epoch 264, loss 0.14\n",
      "epoch 265, loss 0.14\n",
      "epoch 266, loss 0.14\n",
      "epoch 267, loss 0.14\n",
      "epoch 268, loss 0.14\n",
      "epoch 269, loss 0.14\n",
      "epoch 270, loss 0.14\n",
      "epoch 271, loss 0.14\n",
      "epoch 272, loss 0.14\n",
      "epoch 273, loss 0.14\n",
      "epoch 274, loss 0.14\n",
      "epoch 275, loss 0.14\n",
      "epoch 276, loss 0.14\n",
      "epoch 277, loss 0.14\n",
      "epoch 278, loss 0.14\n",
      "epoch 279, loss 0.14\n",
      "epoch 280, loss 0.13\n",
      "epoch 281, loss 0.13\n",
      "epoch 282, loss 0.14\n",
      "epoch 283, loss 0.13\n",
      "epoch 284, loss 0.13\n",
      "epoch 285, loss 0.13\n",
      "epoch 286, loss 0.13\n",
      "epoch 287, loss 0.14\n",
      "epoch 288, loss 0.13\n",
      "epoch 289, loss 0.13\n",
      "epoch 290, loss 0.13\n",
      "epoch 291, loss 0.13\n",
      "epoch 292, loss 0.13\n",
      "epoch 293, loss 0.14\n",
      "epoch 294, loss 0.13\n",
      "epoch 295, loss 0.13\n",
      "epoch 296, loss 0.13\n",
      "epoch 297, loss 0.13\n",
      "epoch 298, loss 0.12\n",
      "epoch 299, loss 0.13\n",
      "epoch 300, loss 0.13\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import dezero\n",
    "import dezero.functions as F\n",
    "from dezero import optimizers\n",
    "from dezero.models import MLP\n",
    "\n",
    "\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "#Spiral을 BigData로 교체하는 것만으로 훨씬 큰 데이터셋을 대응할 수 있음.\n",
    "train_set = dezero.datasets.Spiral()\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(train_set)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # Shuffle index for data\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # 미니배치 꺼내기\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch = [train_set[i] for i in batch_index]\n",
    "        batch_x = np.array([example[0] for example in batch])\n",
    "        batch_t = np.array([example[1] for example in batch])\n",
    "\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # 에포크마다 손실 출력\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터셋 전처리\n",
    "# # 모델에 데이터를 입력하기 전에 데이터를 특정한 형태로 가공할 경우가 많다.\n",
    "# # 초기화 시에는 transform과 target_transform을 새롭게 받는다.\n",
    "# # transform은 입력 데이터 하나에 대한 변환 처리, target_transform 레이블 하나에 대한 변환 처리\n",
    "\n",
    "# class Dataset:\n",
    "#     def __init__(self, train=True, transform=None, target_transform=None):\n",
    "#         self.train = train\n",
    "#         self.transform = transform\n",
    "#         self.target_transform = target_transform\n",
    "#         if self.transform is None:\n",
    "#             self.transform = lambda x: x\n",
    "#         if self.target_transform is None:\n",
    "#             self.target_transform = lambda x: x\n",
    "\n",
    "#         self.data = None\n",
    "#         self.label = None\n",
    "#         self.prepare()\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         assert np.isscalar(index)\n",
    "#         if self.label is None:\n",
    "#             return self.transform(self.data[index]), None\n",
    "#         else:\n",
    "#             return self.transform(self.data[index]),\\\n",
    "#                     self.target_transform(self.label[index])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def prepare(self):\n",
    "#         pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
