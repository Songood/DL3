{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11장\n",
    "\n",
    "모델 저장 및 읽어오기.\n",
    "모델은 추론에 써먹으려고 하는것.\n",
    "학습을 시킨 후 학습에 대한 모델이란것은 파라미터를 최적화시키고 아키텍쳐 정보가 들어가있는것.\n",
    "최적화된 파라미터.(학습이되어서)\n",
    "모델들은 웹, OndeviceAI에 쓰인다.\n",
    "\n",
    "학습을 시켰으면 모델들을 저장하는 방법과 읽어오는것이 필요하다.\n",
    "\n",
    "-드롭아웃\n",
    "모델을 훈련시키다보면 과적합되는 때가 있다.\n",
    "특정한 데이터에 대한 과적합 문제를 해결하기 위한것이 드롭아웃\n",
    "\n",
    "-CNN메커니즘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**53. 모델 저장 및 읽어오기**\n",
    "\n",
    "모델이 가지는 매개변수를 외부 파일로 읽고 저장하고 다시 읽어오는 기능을 만든다.\n",
    "\n",
    "Dezero의 매개변수\n",
    "-Parameter 클래스로 구현\n",
    "-Parameter의 데이터는 인스턴스 변수 data에 ndarrray 인스턴스로 보관\n",
    "-ndarray인스턴스를 외부 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#np.save와 np.load함수들을 사용하여 ndarray인스턴스를 저장하고 읽을 수 있다.\n",
    "x = np.array([1,2,3])\n",
    "np.save('test.npy',x) #ndarray인스턴스를 외부 파일로 저장\n",
    "\n",
    "x = np.load('test.npy') #저장 되어있는 데이터 읽어온다.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 ndarray인스턴스를 저장하고 읽는 방법\n",
    "x1 = np.array([1,2,3])\n",
    "x2 = np.array([4,5,6])\n",
    "\n",
    "np.savez('test.npz', x1=x1, x2=x2) #여러개의 ndarray 인스턴스를 저장, np.savez함수로 정하는 파일 확장자는 .npz\n",
    "\n",
    "arrays = np.load('test.npz')\n",
    "x1 = arrays['x1'] #원하는 키워드 명시하여 해당 데이터를 꺼내온다.\n",
    "x2 = arrays['x2']\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# 넘파이의 코드를 파이썬 딕셔너리를 사용하여 수정\n",
    "\n",
    "x1 = np.array([1,2,3])\n",
    "x2 = np.array([4,5,6])\n",
    "data = {'x1':x1, 'x2':x2}\n",
    "np.savez('test.npy', **data) # 데이터를 저장 # data앞에 **별 두개를 붙여주면 딕셔너리가 자동으로 전개되어 전달됨 \n",
    "\n",
    "arrays = np.load('test.npz')\n",
    "x1 = arrays['x1']\n",
    "x2 = arrays['x2']\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Layer 클래스의 매개변수를 평평하게\n",
    "# ## Layer 클래스의 계층 구조\n",
    "# # 계층은 Layer 안에 다른 Layer가 들어가는 중첩형태의 구조\n",
    "\n",
    "# layer = Layer()\n",
    "\n",
    "# l1 =  Layer() # 레이어 안에 레이어\n",
    "# l1.p1 = Parameter(np.array(1)) #레이어안에 레이어p1\n",
    "\n",
    "# layer.l1 = l1\n",
    "# layer.p2 = Parameter(np.array(2)) #레이어 안에 레이어\n",
    "# layer.p3 = Parameter(np.array(3)) #레이어 안에 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # _flatten_params 메서드\n",
    "# # 인수로 딕셔너리인 params_dict와 텍스트인 parent_key를 받는다.\n",
    "# class Layer:\n",
    "#     def _flatten_params(self, params_dict, parent_key=\"\"):\n",
    "#         for name in self._params:\n",
    "#             obj = self.__dict__[name]\n",
    "#             key = parent_key + '/' + name if parent_key else name\n",
    "\n",
    "#             if isinstance(obj, Layer): # 레이어에서 레이어를 꺼냈을 때 파라미터가 아니라면 재귀적으로 다시 호출\n",
    "#                 obj._flatten_params(params_dict, key)\n",
    "#             else:\n",
    "#                 params_dict[key] = obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 클래스의 save함수와 load함수\n",
    "# def save_weights(self, path):\n",
    "#         self.to_cpu() #메인 메모리에 존재함 보장\n",
    "\n",
    "#         params_dict = {}\n",
    "#         self._flatten_params(params_dict) #평탄화 시킨다.\n",
    "#         array_dict = {key: param.data for key, param in params_dict.items()\n",
    "#                     if param is not None}\n",
    "#         try:\n",
    "#             np.savez_compressed(path, **array_dict)\n",
    "#         except (Exception, KeyboardInterrupt) as e:\n",
    "#             if os.path.exists(path):\n",
    "#                 os.remove(path)\n",
    "#             raise\n",
    "\n",
    "# def load_weights(self, path):\n",
    "#     npz = np.load(path)\n",
    "#     params_dict = {}\n",
    "#     self._flatten_params(params_dict) #값을 꺼내올때\n",
    "#     for key, param in params_dict.items(): #데이터들을 불러온다.\n",
    "#         param.data = npz[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST학습으로 매개변수 저장과 읽기 기능 시험\n",
    "import os\n",
    "import dezero\n",
    "import dezero.functions as F\n",
    "from dezero import optimizers\n",
    "from dezero import DataLoader\n",
    "from dezero.models import MLP\n",
    "\n",
    "\n",
    "max_epoch = 3\n",
    "batch_size = 100\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "model = MLP((1000, 10)) #\n",
    "optimizer = optimizers.SGD().setup(model)\n",
    "\n",
    "# 매개변수 읽기\n",
    "if os.path.exists('my_mlp.npz'): # 1번부터 10000번까지 훈련을 시킬때 MNIST를 10개로 구분. # 기존에 훈련한게 있으면 그걸 로드시켜서 돌리겠다.\n",
    "    model.load_weights('my_mlp.npz')\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss = 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x) #순전파 돌리고\n",
    "        loss = F.softmax_cross_entropy(y, t) #손실함수 y와 t값으로 비교\n",
    "        model.cleargrads()\n",
    "        loss.backward() # 오차\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "\n",
    "    print('epoch: {}, loss: {:.4f}'.format(\n",
    "        epoch + 1, sum_loss / len(train_set)))\n",
    "\n",
    "# 매개변수 저장하기\n",
    "model.save_weights('my_mlp.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**54. 드롭아웃과 테스트 모드**\n",
    "\n",
    "훈련 모델이 99%\n",
    "테스트 데이터로 하니까 정확도가 70%\n",
    ">> 이 결과는 이 모델은 해당 데이터에만 과대적합이 일어난다. \n",
    "\n",
    "#과대적합이 일어나는 요인\n",
    "- 훈련 데이터가 적음\n",
    "- 모델의 표현력이 지나치게 높음\n",
    "\n",
    "#과대적합 해결방법\n",
    "- 데이터를 더 확보하거나 인위적으로 늘리는데 확장을 이용하면 됨\n",
    "- 표현이 높은것은 가중치 감소(표현력이 높은것들을)\n",
    "- , 드롭아웃(노드들을 없앤다), 배치 정규화(배치값이 실질적으로 데이터 플로우 상에서 고르게 분포되어있게 평탄화시킨다) 등\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 드롭 아웃\n",
    "\n",
    "드롭아웃은 학습할 때만 노드를 없앤다.\n",
    "테스트 단계, 추론에서 실제 사용될때는 드롭아웃을 적용하지않는다.\n",
    "\n",
    "- 은닉층(hidden Layer)뉴런을 무작위로 골라 삭제\n",
    "- 삭제된 뉴런은 신호를 전송하지 않는다.\n",
    "- 학습 데이터를 흘려보낼 때마다 삭제할 뉴런을 무작위로 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃 예시\n",
    "import numpy as np\n",
    "\n",
    "dropout_ratio = 0.6\n",
    "x = np.ones(10)\n",
    "\n",
    "# 학습 시\n",
    "mask = np.random.rand(10) > dropout_ratio\n",
    "y = x * mask # mask에서 값이 False인 원소에 대응하는 x의 원소를 0으로 설정.\n",
    "\n",
    "# 테스트 시\n",
    "scale = 1 - dropout_ratio #학습 시에 살아남은 뉴런의 비율\n",
    "y = x * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역 드롭아웃\n",
    "## 스케일 맞추기를 학습할 때 수행\n",
    "\n",
    "\n",
    "# 학습 시\n",
    "scale = 1 - dropout_ratio # dropout_ratio를 동적으로 변경할 수 있다. # 다이렉트 드롭아웃은 dropout_ratio를 고정해두고 학습\n",
    "mask = np.random.rand(10) > dropout_ratio\n",
    "y = x * mask / scale # mask에서 값이 False인 원소에 대응하는 x의 원소를 0으로 설정.\n",
    "\n",
    "# 테스트 시\n",
    "y = x * scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 모드 추가\n",
    "## 역전파 비활성 모드 방식을 유용하게 활용할 수 있음\n",
    "# class Config:\n",
    "#     enable_backprop = True\n",
    "#     train = True\n",
    "\n",
    "\n",
    "# @contextlib.contextmanager\n",
    "# def using_config(name, value):\n",
    "#     old_value = getattr(Config, name)\n",
    "#     setattr(Config, name, value)\n",
    "#         yield\n",
    "#         setattr(Config, name, old_value)\n",
    "\n",
    "# def test_mode():\n",
    "#     return using_config('train', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃 코드 분석\n",
    "\n",
    "# def dropout(x, dropout_ratio=0.5):\n",
    "#     x = as_variable(x)\n",
    "\n",
    "#     if dezero.Config.train:\n",
    "#         xp = cuda.get_array_module(x)\n",
    "#         mask = xp.random.rand(*x.shape) > dropout_ratio\n",
    "#         scale = xp.array(1.0 - dropout_ratio).astype(x.dtype)\n",
    "#         y = x * mask / scale\n",
    "#         return y\n",
    "#     else:\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "variable([2. 0. 2. 0. 0.])\n",
      "variable([1. 1. 1. 1. 1.])\n"
     ]
    }
   ],
   "source": [
    "# 드롭아웃 구현\n",
    "\n",
    "import numpy as np\n",
    "from dezero import test_mode\n",
    "import dezero.functions as F\n",
    "\n",
    "x = np.ones(5)\n",
    "print(x)\n",
    "\n",
    "# 학습 시 \n",
    "y = F.dropout(x)\n",
    "print(y)\n",
    "\n",
    "# 테스트 시\n",
    "with test_mode():\n",
    "    y = F.dropout(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**55.CNN 메커니즘**\n",
    "- 합성곱 신경망은 이미지 인식, 음성 인식, 자연어 처리 등 다양한 분야에서 사용됨\n",
    "- 이미지 인식용 딥러닝은 대부분 CNN기반\n",
    "\n",
    "- CNN의 가장 중요한 구성 요소는 합성곱 층\n",
    "첫 번째 합성곱 층의 뉴런은 합성곱 층 뉴런의 수용장 안에 있는 픽셀에만 연결\n",
    "\n",
    "두 번째 합성곱 층에 있는 각 뉴런은 첫 번째 층의 작은 사각 영역 안에 위치한 뉴런에 연결\n",
    "\n",
    "*CNN 구조\n",
    "- Conv 계층과 Pool계층이 새로 추가되어서\n",
    "- Linear -> ReLU연결이 Conx -> ReLU -> (Pool)로 대체\n",
    "- 출력층에서는 Linear -> ReLU 조합이 사용\n",
    "\n",
    "\n",
    "*합성곱 연산\n",
    "- 이미지 처리에서 말하는 필터 연산에 해당(필터를 커널이라고도 쓴다)\n",
    "- 입력 데이터에 필터를 적용\n",
    "- 형상을 (높이,너비) 순서대로 표기시 입력 형상은 (4,4) 필터는 (3,3) 출력은 (2,2) \n",
    "\n",
    "*연산 계산 순서\n",
    "- 입력 데이터에 대한 필터 윈도를 일정 간격으로 이동하면서 적용\n",
    "\n",
    "*편향을 포함한 합성곱 연산의 처리 흐름\n",
    "- 합성곱층에도 편향이 존재\n",
    "- 편향은 필터링 후에 더해준다.\n",
    "- 편향은 하나 뿐이기도하고 하나의 똑같은 값이 필터 적용 후 모든 원소에 브로드캐스트되어 더해진다.\n",
    "\n",
    "*패딩\n",
    "- 합성곱층의 주요 처리 전에 입력 데이터 주위에 고정값을 채운다.\n",
    "- 기본의 4*4에 패딩 1을 해준다면 4*4 주위로 패딩1이 둘러진다.\n",
    "\n",
    "* 패딩을 하는 이유는 출력크기를 조정!\n",
    "  IF 9*9인 매트릭스가 존재할때 4*4로 합성곱을 한다면 계속 줄다보니까 7*7...5*5...3*3\n",
    "  3*3과 4*4합성곱을 못해주니까 패딩 1을 해주는 순간 4*4하고 합성곱이 가능해진다.\n",
    "세로방향 패딩과 가로방향 패딩을 서로 다르게 설정 할 수 있다.\n",
    "\n",
    "*스트라이드\n",
    "인풋(28*28) MNIST SET을 입력으로 집어넣으면 INPUT과 OUTPUT을 연결.\n",
    "총 결과를 10개로 분류를 한다면 세팅을 할 때 첫번째로 패딩을 세팅. 스트라이드를 2로 설정하면 2만큼 이동하여 계산. \n",
    "결론 7*7에 스트라이드를 2로 설정할때 출력 크기는 3*3이 된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    }
   ],
   "source": [
    "# 출력 크기 계산\n",
    "## 패딩 크기를 늘리면 출력 데이터의 크기가 커지고 스트라이드를 크게하면 반대로 작아진다.\n",
    "\n",
    "# 이건 공식..!!\n",
    "def get_conv_outsize(input_size, kernel_size, stride, pad):\n",
    "    return (input_size + pad * 2 - kernel_size) // stride + 1\n",
    "\n",
    "\n",
    "H, W = 4, 4  # Input size\n",
    "KH, KW = 3, 3  # Kernel size\n",
    "SH, SW = 1, 1  # Kernel stride\n",
    "PH, PW = 1, 1  # Padding size\n",
    "\n",
    "OH = get_conv_outsize(H, KH, SH, PH)\n",
    "OW = get_conv_outsize(W, KW, SW, PW)\n",
    "print(OH, OW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**56.CNN메커니즘(2)**\n",
    "- 2차원 데이터에서의 합성곱 연산\n",
    "수직 및 수평 방향으로 늘어선 합성곱 연산\n",
    "가로/세로 방향\n",
    "\n",
    "- 3차원 데이터의 합성곱 연산\n",
    "이미지 처리를 위해 3차원 데이터를 다뤄야한다.\n",
    "깊이 방향으로 데이터가 늘어난것을 제외하면 필터가 움직이는 방법도 계산도 같다.\n",
    "2차원 합성곱 연산은 대부분의 딥러닝 프레임워크에서 Conv2d라는 이름으로 제공\n",
    "\n",
    "- 합성곱 연산을 블록으로 표현\n",
    "데이터가 (채널,높이,너비) 순으로 정렬 되었다면 형상은 (C,H,W) 필터는 (C,KH,KW)로 표기\n",
    "출력은 특징 맵\n",
    "\n",
    "- 편향을 포함한 합성곱 연산 처리\n",
    "편향은 채널당 하나의 값만 갖는다.\n",
    "편향의 형상은 (OC, 1, 1)이 되고 필터 적용 후의 출력은 (OC, OH, OW)\n",
    "편향은 형상이 다르기 때문에 브로드캐스트된 다음에 더해진다\n",
    "\n",
    "- 합성곱 연산의 미니배치 처리\n",
    "미니배치 처리를 위해서는 각 층을 흐르는 데이터를 4차원 텐서로 취급\n",
    "N개의 데이터로 이뤄진 미니배치 합성곱 연산 수행\n",
    "4차원 텐서의 샘플 데이터 각각에 대해 독립적으로 똑같은 합성곱 연산을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 풀링층\n",
    "풀링은 가로, 세로 공간을 작게 만드는 연산\n",
    "최대 풀링은 최댓값을 취하는 연산이며 2 x 2는 대상 영역의 크기를 나타낸다.\n",
    "일반적으로 풀잉 윈도 크기와 스트라이드 크기는 같은 값으로 설정\n",
    "\n",
    "* 주요 특징\n",
    "    학습하는 매개변수가 없다.(대상 영역에서 최댓값(or 평균값)처리만 하면 끝)\n",
    "    \n",
    "    채널 수가 변하지 않는다.(계산이 독립적으로 이루어진다.)\n",
    "    \n",
    "    미세한 위치 변화에 영향을 덜 받는다.\n",
    "    (1. 입력 데이터의 차이가 크지 않으면 풀링 결과가 크게 달라지지않는다 \n",
    "    2. 입력 데이터의 미세한 차이에 강건\n",
    "   ex. 오른쪽으로 1원소만큼 어긋나지만 출력은 달라지지않는다.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
